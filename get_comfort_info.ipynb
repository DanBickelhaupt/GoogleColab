{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hue1Zmpamx2F"
      },
      "outputs": [],
      "source": [
        "# Given FlightAware aircraft & carrier codes and look-up csv paths, use function get_function_data() to get aircraft interior information from aerolopa.com.\n",
        "\n",
        "# EXAMPLE:\n",
        "# ac_id = 'A321'\n",
        "# carrier_code = 'AAL'\n",
        "# ac_decode = \"ac_codes.csv\"\n",
        "# carrier_decoder = \"carrier_codes.csv\"\n",
        "# aerolopa_decoder = \"aerolopa_links_final.csv\"\n",
        "# comfort_data = get_comfort_data(ac_id, carrier_code, ac_decode, carrier_decoder, aerolopa_decoder)   \n",
        "# seat_width = get_seat_width(comfort_data)\n",
        "\n",
        "# Imports\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Functions\n",
        "def get_html_data(url):\n",
        "    headers = {\n",
        "        \"Accept-Language\" : \"en-US,en;q=0.5\",\n",
        "        \"User-Agent\": \"Defined\",\n",
        "    }\n",
        "    page = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    soup = soup.body\n",
        "    results_all = soup.find_all('div', {'class':'text'})\n",
        "    return str(results_all[1])\n",
        "\n",
        "def remove_bracket_text(string):\n",
        "    start = []\n",
        "    stop = []\n",
        "    removal_pair = []\n",
        "    out = string\n",
        "    for i, letter in enumerate(string):\n",
        "        if letter == '<':\n",
        "            start = i\n",
        "        if letter == '>':\n",
        "            stop = i\n",
        "        if (start != []) and (stop != []):\n",
        "            removal_pair.append((start, stop))\n",
        "            start = []\n",
        "            stop = []\n",
        "    if len(removal_pair) > 0:\n",
        "        idx_adjust = 0\n",
        "        for pair in removal_pair:\n",
        "            out = out[:pair[0]-idx_adjust] + \" \" + out[pair[1]-idx_adjust+1:]\n",
        "            idx_adjust += pair[1] - pair[0]\n",
        "    return out\n",
        "\n",
        "def get_url_data(url):\n",
        "    headers = {\n",
        "        \"Accept-Language\" : \"en-US,en;q=0.5\",\n",
        "        \"User-Agent\": \"Defined\",\n",
        "    }\n",
        "    page = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "    soup = soup.body\n",
        "    results_all = soup.find_all('div', {'class':'text'})\n",
        "    output_final = ''\n",
        "    for results in results_all:\n",
        "        results = list(results)\n",
        "        output_interim = ''\n",
        "        for item in results: \n",
        "            item = str(item)\n",
        "            if item == '<br/>':\n",
        "                output_interim += '\\n'\n",
        "            else:\n",
        "                item = item.replace(\"<p>\",\"\")\n",
        "                item = item.replace(\"</p>\", \"\")\n",
        "                item = item.replace(\"<br/>\", \"\\n\")\n",
        "                output_interim += item\n",
        "        strip_list = ['<strong>', '</strong>', 'strong>', '</strong']\n",
        "        for remove in strip_list:\n",
        "            output_interim = output_interim.replace(remove, '')\n",
        "        output_interim = output_interim.replace('\\n\\n','\\n')\n",
        "        if '=' not in output_interim:\n",
        "            output_final += output_interim\n",
        "    return remove_bracket_text(output_final)\n",
        "\n",
        "def get_link(query, link_df):\n",
        "    # Match query with table entry\n",
        "    bool_list = (link_df['Airline'].str.contains(query[0])) & (link_df['OEM'] == query[1]) & (link_df['AC'].str.contains(query[2]))\n",
        "    if bool_list.sum() > 0:\n",
        "        link = str(link_df['Link'][bool_list].values[0])\n",
        "    else:\n",
        "        link = ''\n",
        "    return link\n",
        "\n",
        "def convert_fa_codes(ac_id, carrier_code, acid_df, carrier_df):\n",
        "    # Get OEM and A/C type from aircraft_id\n",
        "    carrier_code = carrier_code.strip()\n",
        "    ac_id = ac_id.strip()\n",
        "    oem_acmodel = acid_df[acid_df['Code'] == ac_id]['Aircraft Type'].iloc[0]\n",
        "    idx = oem_acmodel.find(' ')\n",
        "    oem = oem_acmodel[:idx]\n",
        "    ac = oem_acmodel[idx+1:]\n",
        "    # Get carrier name from carrier code\n",
        "    carrier_name = carrier_df[carrier_df['Carrier Code'] == carrier_code]['Carrier'].iloc[0].replace(' Airlines', '')\n",
        "    return [carrier_name, oem, ac]\n",
        "\n",
        "def get_comfort_data(ac_id, carrier_code, ac_code_loc, carrier_code_loc, aerolopa_code_loc):\n",
        "    # Load decoder rings\n",
        "    acid_df = pd.read_csv(ac_code_loc, encoding=\"iso-8859-1\")\n",
        "    carrier_df = pd.read_csv(carrier_code_loc, encoding=\"iso-8859-1\")\n",
        "    aerolopa_lookup_df = pd.read_csv(aerolopa_code_loc, encoding=\"iso-8859-1\")\n",
        "    # Convert FlightAware codes into Aerolopa format for query\n",
        "    query = convert_fa_codes(ac_id, carrier_code, acid_df, carrier_df)\n",
        "    # Return plane comfort data\n",
        "    base_url = \"https://www.aerolopa.com/\"\n",
        "    html_suffix = get_link(query, aerolopa_lookup_df)\n",
        "    if html_suffix != '':\n",
        "        comfort_data = get_url_data(base_url+html_suffix)\n",
        "    else:\n",
        "        comfort_data = 'No cabin data available.'\n",
        "    return comfort_data\n",
        "\n",
        "def get_seat_width(string_in):\n",
        "    idx = string_in.lower().rfind('seat width')\n",
        "    start = idx-25\n",
        "    try:\n",
        "        text = string_in[idx-7:idx+17]\n",
        "    except:\n",
        "        try:\n",
        "            text = string_in[:idx+17]\n",
        "        except:\n",
        "            text = string_in\n",
        "    seat_width = []\n",
        "    if idx != -1:\n",
        "        seat_width = re.findall(r'\\d+\\.\\d+', text)\n",
        "        if seat_width == []:\n",
        "            seat_width = re.findall(r'\\d+', text)\n",
        "    if len(seat_width)>0: seat_width = seat_width[-1]\n",
        "    return 17.0 if isinstance(seat_width, list) and len(seat_width) == 0 else seat_width\n",
        "\n",
        "# Test code to make sure webpage scrape request is functional\n",
        "# headers = {\n",
        "#         \"Accept-Language\" : \"en-US,en;q=0.5\",\n",
        "#         \"User-Agent\": \"Defined\",\n",
        "# }\n",
        "# url = \"https://www.aerolopa.com/\"\n",
        "# page = requests.get(url, headers=headers)\n",
        "# soup = BeautifulSoup(page.content, 'html.parser')\n",
        "# print(soup)\n",
        "\n",
        "# Get all aerolopa data\n",
        "aerolopa_decoder = \"aerolopa_links_final.csv\"\n",
        "aerolopa_df = pd.read_csv(aerolopa_decoder)\n",
        "base_url = \"https://www.aerolopa.com/\"\n",
        "scrape_save = []\n",
        "for html_suffix in aerolopa_df[\"Link\"]:\n",
        "    print(html_suffix)\n",
        "    comfort_data = get_url_data(base_url+html_suffix)\n",
        "    scrape_save.append(comfort_data)\n",
        "aerolopa_df[\"Website_Data\"] = np.array(scrape_save).reshape(-1,1)\n",
        "aerolopa_df.to_csv(\"aerolopa_links_final_full.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}